\newpage
\chapter{Data set}
This chapter describes the data set used for evaluation in this thesis. The data set is published at: \href{http://wis.ewi.tudelft.nl/umap2011/}{http://wis.ewi.tudelft.nl/umap2011/} This web page contains material and information about the paper \textbf{Analyzing User Modeling on Twitter for Personalized News Recommendations} \footnote{http://link.springer.com/chapter/10.1007\%2F978-3-642-22362-4\_1\#page-1}. 

This Data set is collected by Fabian Abel and Qi Gao in their paper\cite{Abel2011}.

This page publishes 6 data set in SQL file format:
\begin{itemize}
\item umap-2011-tweets
\item umap-2011-news
\item umap-2011-sementicsTweetsEntity
\item umap-2011-sementicsTweetsTopic
\item umap-2011-sementicsNewsEntity
\item umap-2011-sementicsNewsTopic
\end{itemize}

\section{Introduction of Dataset}
\subsection{Tweets Data Set}
One thousand six hundred and nineteen users’ data collected within a period of two months are included in Data set. Each user has published at least 20 Tweets and at least one Tweet per month. This Data set contains a total of 2,316,204 Tweets. 	

\begin{figure}[ht]
	\centering
	\includegraphics[width=\textwidth]{images/tweets}
	\caption{Number of Tweets Per User}
	\label{NrTwPerUser}
\end{figure}

The collected Tweets Data set includes users’ IDs, Tweet IDs, Tweet content, repost Tweet content, the user ID of the repost Tweet, repost time, and repost Tweet's user ID and so on.

Fields List of Tweets\_sample are listed below:

\begin{longtable}{|c|c|c|}
	\hline \textbf{Field} &  \textbf{Type} &  \textbf{Description} \\ 
	\hline\hline id & bigint &  \\ 
	\hline userId & bigint & ID of user \\ 
	\hline username & varchar & not used. \\ 
	\hline content & varchar & tweets content. \\ 
	\hline creationTime & timestamp & used to filter by time. \\ 
	\hline favorite & tinyint &  \\ 
	\hline replyToPostId & bigint &  \\ 
	\hline replyToUsername & varchar &  \\ 
	\hline retweetedFromPostId & bigint &  \\ 
	\hline retweetedFromUsername & varchar &  \\ 
	\hline retweetCount & int &  \\ 
	\hline latitude & double &  \\ 
	\hline longitude & double &  \\ 
	\hline placeCountry & varchar &  \\ 
	\hline placeCountryCode & varchar &  \\ 
	\hline placeStreetAddress & varchar &  \\ 
	\hline placeURL & varchar &  \\ 
	\hline placeGeometryType & varchar &  \\ 
	\hline placeName & varchar &  \\ 
	\hline placeFullName & varchar &  \\ 
	\hline placeId & varchar &  \\ 
	\hline source & varchar &  \\
	\hline json & text & \shortstack{\\[1em]This field is in JSON format and\\ contains complex information.} \\[1em]
	\hline timeOfCrawl & timestamp &  \\
	\hline crawledViaNewsMedia & tinyint & \shortstack{\\[1em]This tweet may have a\\ corrosponding news if this field is 1.} \\ [1em]
	\hline 
\end{longtable}

\newpage
\subsection{News Data Set}
News: In order to link the tweets with the news articles, more than 60 RSS feeds of prominent news media such as BBC, CNN and New York Times were monitored and 77,544 news articles were extracted.

\newpage
\begin{figure}[ht]
	\centering
	\includegraphics[width=\textwidth]{images/newsPerDay}
	\caption{Number of News Per Day}
	\label{NrNersPerDay}
\end{figure}

\newpage
\begin{figure}[ht]
	\centering
	\includegraphics[width=\textwidth]{images/newsRankingPerDay}
	\caption{Ranking BBC vs. CNN vs. NY Times}
	\label{newsRankingPerDay}
\end{figure}

\newpage
The News includes: News, id, source, url, title, publish-time, content and so on.
\begin{table}[h]
	\begin{longtable}{|c|c|c|}
		\hline \textbf{Field} & \textbf{Type} & \textbf{Description} \\ 
		\hline\hline id & bigint & Index column, ID of news. \\ 
		\hline source & varchar & RSS source. \\ 
		\hline category & varchar &  \\ 
		\hline url & text &  \\ 
		\hline title & varchar &  \\ 
		\hline description & text &  \\ 
		\hline newscontent & longtext &  \\ 
		\hline publish\_date & datetime &  \\ 
		\hline update\_date & datetime &  \\ 
		\hline crawl\_date & timestamp &  \\ 
		\hline 
	\end{longtable}
	\caption{Fields List of News} 
\end{table}
 
\section{Manipulate SQL File}
\subsection{Import dataset into MySQL server}
The original Data set is published as a MySQL SQL file format, so it must be imported to the MySQL server initially. The console command is:

\begin{lstlisting}[language=bash]
mysql -u root -default-character-set=utf8 -p lyc < “sql file path”
\end{lstlisting}

Note that the -default-character-set=utf8 argument must not be removed. Otherwise, it can not be imported correctly. 

Database Management Tools such as PhpMyAdmin or NaviCat may not work properly, because the SQL files are too large.

\subsection{Export CSV files}
Flink is widely used to deal with the CSV files. Since direct manipulation of a SQL database is not good practice\footnote{Suggestion for Flink: DataSet from SQL Server \ref{Flink:SQLServerDataSet}}， all the original files need to be transformed as CSV files. This operation can be done with the help of PhpMyAdmin. One can run a SQL query and export the query results, thus making it possible to export only the useful columns of a table.

\subsubsection{Handle CRLF Characters}
Flink has a disadvantage, it can not handle CRLF characters correctly in the field. \footnote{Suggesion for Flink: CRLF \ref{Flink:CRLF}} 

To solve this problem, PhpMyAdmin for exporting the CSV files. Therefore, if the exported fields contain any CRLF characters,  the option “Remove carriage return/line feed characters within columns” must be checked.

And the max\_execution\_time argument of PHP must be reset, if the export file is too large (i.e., over 2 GB). 

The default row delimiter of Flink can not recognise CRLF in the fields; therefore, all the CRLF should be placed. 

At first, PhpMyAdmin was used to delete all the CRLF characters. However, it was not successful because in some lines, there were no punctuations at the end, which leads to a meaningless new word from the integration of the last word of a line and the first word of the next, such as:

\begin{quote}
	David Millar\\
	But British world time
\end{quote}


If we substitute CRLF,it will becomes：

\begin{quote}
	David MillarBut British world time
\end{quote}

\textit{MillarBut} becomes a new word without a meaning. 

Therefore, the order of derived news SQL is: 

\begin{lstlisting}[language=SQL]
SELECT `id`, `url`, REPLACE(`title`, '\n', ' ') as `title`,
	REPLACE(`description`, '\n', ' ') as `description`, 
	REPLACE(`newscontent`, '\n', ' ') as `newscontent`, 
	`publish_date`, `crawl_date` 
FROM `news`
\end{lstlisting}

So, a Java class ExportCsv is written to export the CSV files. Here the CRLF character is replaced by a blank space.

\subsubsection{Remove Other Trash Data}
The \textit{content} field of \textit{tweets\_sample} includes @<user name>, \#<group name>, HTML Tags and url data; the \textit{newscontent} field of \textit{news} table includes HTML Tags and URL. These data should be removed while exporting.
Therefore, after several attempts,it was decided to use regular expressions. It as follows:
Therefore, the \textit{newscontent} field in \textit{news} contains also advertisement: 

\begin{quote}
Please turn on JavaScript. Media requires JavaScript to play. 
Advertisement
\end{quote}

The advertisements are crawled from web pages, and they should be removed before computing.

Therefore, I experienced several ways. At the end, I decided to achieve this task by regular expression. My regular expression is: 

\begin{lstlisting}[language=Rexx]
(((https?|ftp|gopher|telnet|file|Unsure|http):((//)|(\\\\))+.*?) )|(#.*? )|(@.*? )|(((https?|ftp|gopher|telnet|file|Unsure|http):((//)|(\\\\))+.*?)$)|(#.*?$)|(@.*?$)|(<.*?>)|[¶“”'`´’‘\\?]|(Please turn on JavaScript. Media requires JavaScript to play.( Advertisement)?)
\end{lstlisting}

\subsubsection{Deserialize `json` field}
In Tweets Data set, there is no clear field pointing at the News dataset. Therefore, the link between the two Data sets must be established first. If the user reposts a piece of News on Twitter, it might be understood that this user clicked on a News link.

In general, the URL of News exists in the`title` and the`json` fields of the`tweets\_sample` table. Some rows are abnormal and contain no URL; these rows can not be matched and are ignored.

After careful observation, it was noted that the URL in the `json` field is much more regular than the `title`, but it is wrapped within the JSON formatted data. These URL data must be extracted from it.
Gson is a Java based JSON serialization / deserialization library. it is used to deserialize the `json` field.

Some tweets contain multiple URLs, but mostly the last URL is the correct one.

\subsubsection{File path}
Finally, the data is exported to a CSV file. The CSV file must be placed in the right place. The ROOT folder of the data set is defined in the  \textit{config.json} file. The CSV files must be renamed as \textit{tweets.csv} and \textit{news.csv} and placed in the src folder under the ROOT folder.

\section{Categories, Entities and Topics}
The table `news` contains the`category` field. All distinct `categories` are listed below:

\begin{quote}
NULL, asia, BBC6MusicNews, BBCBusiness, BBCClick, bbccomedy, bbchealth, bbcscitech, BBCSouthAsia, bbcsport\_ticker, BBCTech, BBCWorld, cityroom, CNN, cnn\_tech, CNNLive, CNNPolitics, CNNshowbiz, CNNTopStories, CNNWorld, debugging, europe, nyt\_tech, NYTimesAd, nytimesarts, nytimesbits, nytimescollege, nytimesdining, nytimeseconomix, nytimesfashion, nytimesgoal, nytimeshealth, NYTimesHome, nytimesideas, nytimesjobs, nytimesmagazine, nytimesmusic, nytimesnational, nytimesphoto, nytimespolitics, nytimesscience, nytimesslapshot, nytimesstyle, nytimestech, nytimestheater, nytimesthepour, NYTimesTierney, nytimestravel, nytimestv, nytimeswheels, nytimesworld, NYTMainGlobal, NYTMainUS, NYTpersonaltech, paper\_cuts,sport, thecaucus, thelede, Your\_Money
\end{quote}

They are clearly irregular. Therefore the `category` field is not suitable for data relationship calculation.

However, the Data set host also provides another four different Data sets and they are: Entities and Topics each from news and tweets. It is found that such files are more suitable for the next step.

The distinct entities are listed below:

\begin{quote}
Anniversary, City, Company, Continent, Country, Currency, EmailAddress, EntertainmentAwardEvent, Facility, FaxNumber, Holiday, IndustryTerm, MarketIndex, MedicalCondition, MedicalTreatment, Movie, MusicAlbum, MusicGroup, NaturalFeature, OperatingSystem, Organization, Person, PhoneNumber, PoliticalEvent, Position, Product, ProgrammingLanguage, ProvinceOrState, PublishedMedium, RadioProgram, RadioStation, Region, SportsEvent, SportsGame,  SportsLeague, Technology, TVShow, TVStation, URL
\end{quote}

Distinct topics are listed below:

\begin{quote}
Business\_Finance, Disaster\_Accident, Education, Entertainment\_Culture, Environment, Health\_Medical\_Pharma, Hospitality\_Recreation, Human\_Interest, Labor, Law\_Crime, Other, Politics, Religion\_Belief, Social Issues, Sports, Technology\_Internet, War\_Conflict, Weather
\end{quote}

\subsection{Statistics of Topics}
Unfortunately, not all the news have corresponding topics.

\begin{lstlisting}[language=SQL]
SELECT * FROM news WHERE id NOT IN (SELECT newsId FROM `semanticsNewsTopic`)
\end{lstlisting}

The number of news without topics : 14946.

\begin{lstlisting}[language=SQL]
SELECT * FROM tweets\_sample
WHERE id not in 
	(SELECT tweetId FROM `semanticsTweetsTopic`)
\end{lstlisting}

The number of tweets without topics: 1631377.

\begin{lstlisting}[language=SQL]
SELECT COUNT(*)
FROM tweets_sample 
WHERE crawledViaNewsMedia = 1 
	AND id not in 
		(SELECT tweetId FROM `semanticsTweetsTopic`)
\end{lstlisting}

The number of no-topic tweets with news: 37944.

Though topics is a helpful field for recommendation, it does not cover all data.

\section{Build relationships between tweets and news}

\subsection{Filtering out the rows with ‘1’ in crawledViaNewsMedia}
\textit{crawledViaNewsMedia} is a boolean field, means whether the tweet retweeted a news. If the user reposts a piece of News on Twitter, we will think that this user click on the News link. 

This filter output amounted to only 49203, whereas the total amount of the \textit{tweets\_sample} was 2316204. This ratio is 49203/2316204 = 2.12\%, which is too low.

Besides, the total amount of involved users is 1619. That is, each user reposts at least one piece of news. The total number of news posts are 77855, so it is necessary that a lot of news could not be found with its corresponding items.

Since there is no url value in some of the repost news, the link between the title and the news must be established.

Therefore, all the repost news must be first extracted based on the crawleViaNewsMedia’s value in the Tweets dataset.

\subsection{Deserialize json field}
The \textit{join} method was used to correspond the title and the url. Only used the title or the url was not appropriate, because the data was incomplete. In the \textit{tweets\_sample}, \textit{title} and \textit{json.entities.URLs[0].URL} might contain url information. This paper used \textit{json.entities.URLs[0].URL}. As long as the title or the url could be corresponded, the link could be constructed between the tweets and news.  

The \textit{join} method is used to correspond the title and the url. Only using the \textit{title} or the \textit{url} field is not appropriate because the data is incomplete. In the \textit{tweets\_sample}, \textit{title} and \textit{json.entities.URLs[0].URL} might contain url information. This paper uses \textit{json.entities.URLs[0].URL}. As long as the \textit{title} or the \textit{url} can be corresponded, the link can be constructed between the tweets and the news.

\subsection{Data relationships}
The URLs in \textit{tweets\_sample} and \textit{news} are taken as the key values, and the two data sets are merged. The join function of Flink is very convenient to process this task.

Since there are no websites of repost news in some Tweets data sets, the title is taken as the key value to obtain the link. The code is presented as below:

\begin{lstlisting}[language=Java]
tweetsDataSet.filter(new FilterFunction<TweetModel>() {
		@Override
		public boolean filter(TweetModel tweetsModel) throws Exception {
			// Match 1/10 users each invoke.
			return tweetsModel.userId % 10 == group;
		}
	})
	.cross(newsModelDataSet)
	.filter(new FilterFunction<Tuple2<TweetModel, NewsModel>>() {
		long counter = 0;

		@Override
		public boolean filter(Tuple2<TweetModel, NewsModel> input) throws Exception {
			if ((input.f0.url_good && input.f1.url != null &&
				input.f0.url.equals(input.f1.url)) ||
				(input.f0.title != null && input.f1.title != null &&
					!input.f0.title.isEmpty() && !input.f1.title.isEmpty() &&
					input.f0.title.equals(input.f1.title))) {
				return true;
			}
		}
	});
\end{lstlisting}

\section{The problems in news join tweets}

\subsection{Problem case 1: Zombie News}

The content of tweets with ID 3114711084171264 is:

\begin{quote}
RT @nytimes:Henryk Mikolaj Gorecki, Composer, Dies http://nyti.ms/a7xN0I
\end{quote}

The URL could be used to match. And find out its corresponding News and its ID is 27187, but this News did not have source, category, title, publish\_date or description, which had the content only. Its content was: 

\begin{quote}
Not an NYTimes.com member yet? Just answer a few simple questions, select an ID and password and you'll be all set. (the following contents will be omitted)
\end{quote}

Obviously, when the web crawler was crawling this data, it lost session causing crawl errors.

\textit{http://nyti.Ms} was a website devoted to the NYTimes.com as a website thumbnail, unfortunately some of these websites no longer corresponded correctly.  The title "Henryk Mikolaj Gorecki, Composer, Die" could be searched on the Internet, and this article about NYTimes.com was found:

http://www.nytimes.com/2010/11/13/arts/music/13gorecki.html

This was the only related report on NYTimes.com, released at 2010-10-13. But the release time on Tweets was at 16:59:39 on 2010-11-12. This paper speculated the reason that NYTimes.com published an article initially, tweets repost it, but NYTimes.com deleted this article later and post it again, whereas the corresponding content on tweets was not updated again.

This paper was unable to examine accurately the amount of similar errors, which were a lot judging from the experiences. 

\subsection{Problem case 2: Similar but not matched.}

Sample tweets ID: 5601280391118848, its content is:

\begin{quote}
\#German minister: Suspicious bag found in \#Africa was "U.S. test device.
\end{quote}

Its URL is \textit{http://www.cnn.com/}

In the news, five corresponding items could be found. But they did not make any sense, since \textit{http://www.cnn.com/} referred to a domain name rather than an article. Five similar items could be found if they were retrieved directly in the news manually: 

\begin{description}
	\item \url{http://rss.cnn.com/~r/rss/edition_europe/~3/k6kpw_xfap4/index.html}
	\item \url{http://edition.cnn.com/2010/WORLD/europe/11/18/germany.suspicious.luggage/index.html?eref=edition}
	\item \url{http://rss.cnn.com/~r/rss/cnn_world/~3/6dXHRdknv8U/index.html}
	\item \url{http://rss.cnn.com/~r/rss/cnn_topstories/~3/MxFEbwAOM-I/index.html}
	\item \url{http://www.cnn.com/2010/WORLD/europe/11/18/germany.suspicious.luggage/index.html?eref=rss_topstories&utm_source=\%23frankguillen&utm_medium=twitter&utm_campaign=FrankGuillen+Buzz}
\end{description}

After reading carefully, these five items were found not relevant. These five news were the same, which all reported the suspicious package was found at Namibia airport in Germany. However, the contents in the tweets were the follow-up report about these five pieces of news, which called the "suspicious package" as a testing supply in the United States.

Further observations could be found that three website were rss.cnn.com. RSS is Rich Site Summary, which is an aggregation of news contents.

In fact, a large amount of URL was RSS in the news, which brought a lot of interference to the corresponding work.

\subsection{Problem case 3: Multiple matches}

Sample tweets ID: 7338529600315393\\
Title: Apple-1 computer sells for \$213,000

Two pieces of news were found for this tweet by matching the titles, a piece of news was found by matching the URL, they were listed as below:\\[1em]

\begin{tabular}{|c|c|c|}
	\hline News ID & URL & Title \\ 
	\hline 15268 & \shortstack{\href{http://rss.cnn.com/~r/rss/cnn_topstories/~3/5Mhop8Rha3Q/index.html}{http://rss.cnn.com/.../index.html}} & Apple-1 computer sells for \$213,000 \\
	\hline 15366 & \shortstack{\href{http://rss.cnn.com/~r/rss/cnn_tech/~3/3DG5l4HB31k/index.html}{http://rss.cnn.com/.../index.html}} & Apple-1 computer sells for \$213,000 \\
	\hline 19354 & \url{http://on.cnn.com/hQEK0G} & null \\
	\hline
\end{tabular} 
\\[1em]

The first two were the RSS links obviously, which were opened in the browser pointing to the same article indeed, which was found out by matching the websites in the following. 

\subsection{Problem case 4: News Date}
There are plenty of news that is published on 1970-01-01. But it does not mean that the tweet users like old news or that the year 1970 had some breaking news. It is just because the short date format begins at 00:00:00 hrs on 1970-01-01. If the date cannot be crawled correctly, it is marked as 1970-01-01 00:00:00.

Therefore, for the news in 1970 the actual \textit{publish\_date} is unknown, so in this paper we apply a compromised method that adds news \textit{crowed\_time} as \textit{publish\_date} for these pieces of news. 

Therefore, first of all, the link between the URL and the news must be established so as to decompose the \textit{json} field in the tweets.

\section{Match results}
The statistics of match between tweets and news is listed below:

\input{t1-MatchStatistics}

It's obviously that the date 2010-11-24, 2010-11-25, 2010-11-26 and 2010-11-28 contain most data.

The statistics shows that this data set is not a good choice for recommendations evaluation.

\section{Summary}
This chapter describes the datails of the data set to be evaluated laterly. The data set contains a \textit{tweets\_sample} table and a \textit{news} table, 